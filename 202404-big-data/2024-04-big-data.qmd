---
title: "Wrangling larger-than-memory data in R"
author: "Renata Diaz"
format: 
  uaz-revealjs:
      logo: "_extensions/cct-datascience/uaz/logo.png"
editor: source
execute:
  freeze: true  
---

## ⏰ Schedule ⏰ {.smaller}

::: notes
remember to record
:::

| Topic                                    | Time          |
|:-----------------------------------------|:--------------|
| Welcome & intros                         | 11:00 - 11:05 |
| (Really) big data in R: the landscape    | 11:05 - 11:10 |
| Data storage and access with `arrow`     | 11:15 - 11:30 |
| `dplyr` wrangling with `arrow`           | 11:35 - 11:50 |
| BREAK                                    | 11:50 - 12:00 |
| Data storage and wrangling with `duckdb` | 12:00 - 12:20 |
| `arrow` and `duckdb`                     | 12:20-12:25   |
| When to use what                         | 12:25-12:30   |
| Side quests and wrap up                  | 12:30-1       |

# Welcome!

## CCT Data Science Team

#### Supporting reproducible research, data pipelines, and software development in ALVSCE

::: incremental
-   Workshops
-   Drop-in hours
-   "Incubator" program
-   Longer-term collaborations
-   Learn more: <https://datascience.cct.arizona.edu>
:::

## Before we get started

::: incremental
-   These slides (and links!) are available at: NEW LINK HERE
-   Please make sure you have R and RStudio installed
:::

## Background assumptions

::: incremental
-   You do at least some of your data wrangling + analysis in R
-   You use `dplyr` and `tidyr` (and you like it!)
-   Some of your datasets are too large to handle efficiently in R
:::

## Learning objectives

::: incremental
-   Understand the landscape of tools available for big data in R
-   Store and access datasets using the `arrow` and `duckdb` packages
-   Scale up `dplyr` pipelines using `arrow` and `duckdb`
-   Understand which tools are suited to which tasks, and where to look for more resources
:::

# Big data in R

## The (abbreviated) landscape

![](options_files/figure-commonmark/unnamed-chunk-2-1.png){width="100%"}{fig-alt="A plot placing different options for data wrangling in R along axes of ease of use and size of data. dplyr and tidyr are easy to use but can only handle small datasets. External data wrangling tools, and DBI plus SQL, can handle big data but are hard to use. arrow and duckdb can handle large amounts of data and are easy to use."}

## Apache arrow

<https://arrow.apache.org/>

![](images/clipboard-1025653811.png)

## Apache arrow

::: incremental
-   **Format** (columnar data storage = fast!)
-   **Libraries** to interface with many programming languages/platforms
-   Emphasis on **interoperability**
-   `arrow` libraries work with many data formats - you do not need to use parquet/arrow to get performance gains!
:::

## arrow R library

<https://arrow.apache.org/docs/r/>

![](images/clipboard-1388851838.png)

## duckdb

<https://duckdb.org/>

![](images/clipboard-2911850815.png)

## duckdb

::: incremental
-   Database format + API
-   "SQLite for analytics"
-   Complements arrow
-   Handles **relational** data more naturally than arrow
:::

## duckdb and R

<https://duckdb.org/docs/api/r.html>

![](images/clipboard-2016618240.png)

## 

::: callout-important
You can use either, or both!

Today we will cover arrow and duckdb separately and then show how to combine them.
:::

# Today's data: the Breeding Bird Survey

-   Annual counts of birds observed along \>2500 monitoring routes throughout United States and Canada.
-   These aren't **big** big data, but are big enough to show performance gains.

# Today's data: the Breeding Bird Survey

-   Download for today:
-   Source: Ziolkowski Jr., D.J., Lutmerding, M., Aponte, V.I., and Hudson, M-A.R., 2022, North American Breeding Bird Survey Dataset 1966 - 2021: U.S. Geological Survey data release, https://doi.org/10.5066/P97WAZE5.

# Loading data using arrow

## `read.csv` vs. `read_csv_arrow`

```{r, echo = F}

library(tictoc)
library(arrow)
library(dplyr)
```

::: columns
::: {.column width="50%"}
```{r, cache = TRUE, echo = TRUE, include = T}

tic()

birds_csv <- read.csv(
  here::here("202404-big-data", 
             "data",
             "all_routes_all_stops.csv"))

toc()

```
:::

::: {.column width="50%"}
```{r,  echo = TRUE, include = T}


tic()

birds_arrow_csv <-
  arrow::read_csv_arrow(
    file = here::here("202404-big-data",
               "data",
               "all_routes_all_stops.csv"))

toc()

```
:::
:::

## `read.csv` vs. `read_csv_arrow`

::: columns
::: {.column width="50%"}
```{r,  echo = TRUE, include = T}

birds_csv |>
  nrow()

birds_csv |>
  colnames()
```
:::

::: {.column width="50%"}
```{r,  echo = TRUE, include = T}

birds_arrow_csv |>
  nrow()

birds_arrow_csv |>
  colnames()

```
:::
:::

## `read.csv` vs. `read_csv_arrow`

::: callout-important
`read_csv_arrow` is much faster than `read.csv`.

Both functions **read data into memory** and won't work if your data is too big for R to load.
:::

## `read.csv` vs. `open_dataset`

::: columns
::: {.column width="50%"}
```{r, cache = TRUE, echo = TRUE, include = T}

tic()

birds_csv <- read.csv(
  here::here("202404-big-data", 
             "data",
             "all_routes_all_stops.csv"))

toc()

```
:::

::: {.column width="50%"}
```{r,  echo = TRUE, include = T}


tic()

birds_arrow_dataset <-
  arrow::open_dataset(
    sources = here::here("202404-big-data",
               "data",
               "all_routes_all_stops.csv"),
    format = "csv")

toc()

```
:::
:::

## `read.csv` vs. `open_dataset`

::: columns
::: {.column width="50%"}
```{r,  echo = TRUE, include = T}

birds_csv |>
  nrow()

birds_csv |>
  colnames()
```
:::

::: {.column width="50%"}
```{r,  echo = TRUE, include = T}

birds_arrow_dataset |>
  nrow()

birds_arrow_dataset |>
  colnames()

```
:::
:::

## `read.csv` vs. `open_dataset`

::: columns
::: {.column width="50%"}
```{r,  echo = TRUE, include = T}

birds_csv |>
  nrow()

birds_csv |>
  colnames()
```
:::

::: {.column width="50%"}
```{r,  echo = TRUE, include = T}

birds_arrow_dataset |>
  nrow()

birds_arrow_dataset |>
  colnames()

birds_arrow_dataset |>
  head() |>
  collect() |>
  colnames()

```
:::
:::

## `read.csv` vs. `open_dataset`

::: callout-important
`open_dataset` connects to the data but **does not pull it into memory**.

Computations happen with `arrow`'s C++ library.

To get the **results** into R, use `collect()`.
:::

## Hive partitioning

::: columns

::: {.column width="50%"}

![](images/clipboard-2806566351.png)

:::

::: {.column width="50%"}


![](images/clipboard-1412599253.png)

:::
:::

## Hive partitioning

```{r, echo = T}

birds_arrow_hive <-
  arrow::open_dataset(
    sources = here::here("202404-big-data",
               "data",
               "hive"),
    format = "csv",
    partitioning = "StateNum")

birds_arrow_hive |>
  nrow()


birds_arrow_hive |>
  head() |>
  collect() |>
  colnames()

```

## Hive partitioning

:::callout-important

Hive partitioning breaks a dataset into many smaller files based on one or more variables.

You can recombine them with `open_dataset` and it will behave the same as one big file.

This can help with portability.

:::

## `dplyr` pipelines in `arrow`

Supported:

* Filter
* Mutate
* Rename
* Sum 
* Group by summarize
* Joins

Show compute vs collect

Show non-supported function

## `dplyr` pipelines in `arrow`

:::callout-important

Many `dplyr` functions have arrow bindings.

Some have close cousins.

Some are not implemented.

For details, see the [data wrangling vignette](https://arrow.apache.org/docs/r/articles/data_wrangling.html) and list of [supported dplyr functions](https://arrow.apache.org/docs/r/reference/acero.html) in the documentation. 

:::

:::callout-tip

## What do I do if a function isn't supported?

Stay tuned...

:::
# Break

# `arrow` + `duckdb`: Stronger together

## `duckdb`: a reminder

::: callout-note
`duckdb` is a **database format** with an **R API**.
:::

## `duckdb` with `arrow`
::: incremental

* The `duckdb` API can implement some functions that arrow can't.
* You can pass data between `duckdb` and `arrow` to execute functions from either package with minimal additional overhead. 

:::

## `arrow::to_duckdb`

# `duckdb` as an alternative to `arrow`

# `duckdb` alone

::: incremental
* You can use `duckdb` to wrangle data stored as a .duckdb database.
* `duckdb` can open csv, parquet, and many other file formats
* `duckdb` can be configured to load data over HTTPS
* Unlike `arrow`, `duckdb` naturally supports multiple related tables stored within one database
:::

# `duckdb` and `R`: options

::: incremental

* `duckdb` integrates with the `DBI` and `dbplyr` packages to support `dplyr` verbs
* The `duckplyr` package provides a more direct interface to `duckdb`, but is brand-new
:::

# `dplyr` pipelines with `duckdb`

# Non-dplyr operations with `duckdb`

# In summary

## Choosing your tools

| `arrow`                             | `duckdb`                  | `DBI`                           |
|------------------------|------------------------|------------------------|
| Focus on one big table              | Multiple related tables   | Specialized/advanced operations |
| `dplyr`-focused workflow            | `dplyr` + additional verbs       |                                 |
| High value on data interoperability | .duckdb format acceptable |                                 |

#### ✨ You can use multiple tools within one workflow!✨

# Resources

## `arrow`

## `duckdb`

## CCT Data Science opportunities

::: incremental
-   Drop-in hours - Tuesdays, 9-10
-   Incubator projects
-   Upcoming workshops
:::

## Data science \@ UA

::: incremental
-   [Research Bazaar Arizona](https://researchbazaar.arizona.edu/)
-   Coffee & Code
-   Hack Hour
-   UA Data Science Slack
-   Sign up here: <https://jcoliver.github.io/uadatascience-slack/user-guide.html>
-   [UA Libraries Data & Viz Drop-In Hours](https://libcal.library.arizona.edu/event/10712327?_gl=1*23qnh9*_ga*MTcwMzYxNDQwNC4xNjk4MTYyODA2*_ga_7PV3540XS3*MTcwMTgwODgyNy44My4xLjE3MDE4MDk0OTUuMjkuMC4w)
-   UA Data Science Institute
:::

# Thank you!

Thank you for attending!

For details on drop-in hours, upcoming workshops, and Incubator Program offerings from CCT-Data Science, see our [website](https://datascience.cct.arizona.edu)!
