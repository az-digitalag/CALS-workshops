---
title: "Data Validation in Excel and R"
author: "Eric R. Scott"
date: "2022-08-24"
format: revealjs
bibliography: references.bib
---

```{r setup}
options(pillar.width = 75)
```

## Learning Objectives

::: incremental
-   Understand best practices for entering data and fixing errors in data

-   Use Excel data validation tools to prevent data entry errors

-   Compare data entered by two people in R to check for data entry mistakes

-   Explore data summaries to check for errors

-   Get the gist of how you can use the `pointblank` package to perform data validation checks
:::

. . .

::: callout-important
## We are only interested in fixing verifiable mistakes! It is generally **not** appropriate to remove or edit outliers---extreme values that may or may not be accurate.
:::

## Software needed

You'll need access to Excel, R, RStudio, and the following R packages:

```{r}
#| output: false
#| echo: true
library(tidyverse)
library(visdat)
library(pointblank)
library(skimr)
```

If you can, install the development version of `pointblank` using this R code:

``` r
if(!require("remotes")) {
  install.packages("remotes")
} 
remotes::install_github("rich-iannone/pointblank")
```

## Data Entry in Spreadsheets

Spreadsheets are convenient for data entry, but can result in errors.
Take some time setting up your data entry spreadsheet.

::: incremental
-   Use consistent values for categorical variables
-   Explicitly record missing data instead of leaving blanks
-   Use consistent, concise, descriptive, machine-readable column headers (this is hard!)
-   Use YYYY-MM-DD format for dates
:::

::: aside
For more detailed guidelines on using spreadsheets for data entry, I highly recommend [Data Organization in Spreadsheets](https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375989) by @broman2018.
:::

## Use Excel's Validation Tools

Set up validation tools in your data entry spreadsheet to stop data entry errors in their tracks!

![](images/validation-dialog.png){fig-alt="Data Validation dialog box in Excel.  Settings tab is shown with dropdown menu for Allow:" fig-align="center" width="300"}

::: {text-align="center"}
\[DEMO\]
:::

## Watch out for Excel Autocorrect!

::: {layout-ncol="2"}
[![](images/nature-excel.png){fig-alt="Nature headline: \"Autocorrect errors in Excel still creating genomics headache.  Despite geneticists being warned about spreadsheet problems, 30% of published papers contain mangled gene names in supplementary data." width="530"}](https://www.nature.com/articles/d41586-021-02211-4)

![](images/excel-meme.jpg){fig-alt="Confused anime guy with butterfly meme where the guy has a Microsoft Excel logo on his face, the butterfly is \"any data at all\" and the caption is \"is this a date?\"" width="200"}
:::

1.  Explicitly set all column types to numeric, text, date, etc.

2.  Make sure no columns are set to "general"

## Double-entry Method

-   Two people enter the same data, then compare programatically.

-   In the `data` folder, there are two versions of a dataset---one entered by Eric and one entered by Jessica.

```{r}
#| echo: true
eric <- read_csv("data/tea_eric.csv")
jessica <- read_csv("data/tea_jessica.csv")
```

::: callout-note
This example dataset is modified from [@scott2021].
Each day, researchers counted the number of leafhoppers (`hoppers`) on tea plants and measured the length of 7 labeled tea shoots in cm (recorded in `shoot_*` columns).
`leaves` is how many leaves were searched for leafhoppers on each plant and `counter` is the initial of the researcher who took measurements.
:::

## Data Example

::: columns
::: {.column width="60%"}
-   Modified from [@scott2021].

-   Every day, around 6am, measured 7 marked tea shoots per plant

-   Also counted number of leafhoppers on a certain number of leaves

-   Three researchers recorded data (W, G, and E)
:::

::: {.column width="40%"}
![](images/tea.jpg){fig-align="right" width="425"}

![](images/leafhopper.jpg){fig-align="right" width="425"}
:::
:::

## Compare visually with `visdat`

`vis_compare()` visually compares two datasets of the same dimensions.

```{r}
#| warning: false
#| echo: true 
#| fig-align: center
#| fig-width: 7
#| fig-height: 3.5
vis_compare(eric, jessica)
```

## Compare data frames with `anti_join()`

The `dplyr` package includes a set of `*_join()` functions for combining two datasets with some shared columns.

`anti_join()` takes two data frames and returns only rows that differ.
This is perfect for the double-entry method.

```{r}
#| eval: false
#| echo: true
#values in `eric` that are different in `jessica`:
anti_join(eric, jessica)

#values in `jessica` that are different in `eric`:
anti_join(jessica, eric)
```

## Using `anti_join()`

First, I like to add row numbers that match the ones in Excel to make it easier to find the mistakes.

```{r}
#| echo: true
#| code-line-numbers: "1,2|3,4|5"
#| output-location: fragment
# add rownumbers that match Excel (headers are row 1)
excel_rows <- 2:(nrow(eric) + 1)
eric    <- eric    |> mutate(row = excel_rows, .before = date)
jessica <- jessica |> mutate(row = excel_rows, .before = date)
head(eric, 5)
```

## Inspect the output of `anti_join()`

```{r}
#| echo: true
#| df-print: paged

# Rows in eric that don't match jessica
anti_join(eric, jessica)

# Rows in jessica that don't match eric
anti_join(jessica, eric)
```

# Break

## We fixed the data entry mistakes!

-   Data entry errors revealed by double-entry are fixed in `tea_resolved.csv`

```{r}
#| echo: true

tea <- read_csv("data/tea_resolved.csv")
```

-   From this point on, "mistakes" in data are harder to verify

-   Can still take steps to validate the data

## Explore data summaries

::: incremental
-   You can't check for errors if you don't get to know your data!
-   Use `skimr::skim()` to get a nicely formatted summary
-   Look for number of unique values for categorical variables
-   Look for long tails or strange patterns in mini-histograms for numeric variables
:::

## `skim()` the data {.smaller .scrollable}

```{r}
#| echo: true

skimr::skim(tea)
```

## Explore data visually

-   `visdat::vis_guess()` can help spot inconsistencies
-   I'll change one of the plots to a number to demonstrate

```{r}
#| echo: true

#change field in the 10th row to "1"
tea$field[10] <- 1
#doesn't change the type of the column
class(tea$field)
```

## Explore data visually

```{r}
#| echo: true 
#| fig-align: center
#| fig-width: 7
#| fig-height: 4

visdat::vis_guess(tea)
```


## Data validation pipelines with `pointblank`

```{r}
library(pointblank)
```

-   `pointblank` provides 6 (six!) workflows for validating data

-   The [Data Quality Reporting Workflow](https://rich-iannone.github.io/pointblank/articles/VALID-I.html) (VALID-1) is probably most useful for this group

-   Start with a data frame, create an "agent", tell it what to expect of your data with validation functions, and let it "interrogate" your data

-   Output is a HTML table with buttons to download CSV files of any data that didn't pass your validations

### `pointblank` demo

1.  Decide on "action levels". Can set a number or fraction of rows as a threshold for a warning or error

```{r}
al <- action_levels(warn_at = 1, stop_at = .05)
al
```

2.  Create agent

```{r}
agent <- 
  create_agent(
    tbl = tea, #our data example from before
    actions = al
  )
```

3.  Specify validation conditions

    -   Basic checks on column types with `col_is_*()` functions
    -   Check column values with `col_vals_*()` functions
    -   Check rows (e.g. duplicate rows) with `rows_*()` functions

```{r}
agent_informed <- 
  agent |> 
  col_is_character(c(field, counter)) |> #should be character
  col_is_date(date) |> #should be a date
  
  col_vals_in_set(counter, c("W", "G", "E")) |> #The three researchers
  col_vals_lt(  #expect shoots < 15cm
    columns = starts_with("shoot_"), 
    value =  15,
    na_pass = TRUE
  ) |> 
  rows_distinct(columns = vars(plant_id, date)) #no duplicate plant IDs
```

::: callout-important
`pointblank` historically used the `vars()` function to specify columns (e.g. `col_is_character(vars(field))`), but is moving toward a more tidyverse-like approach.
However, at the time of writing this, some functions and arguments still require `vars()` like `rows_distinct()` above.
:::

4.  Interrogate!

```{r}
#| column: body-outset

agent_informed |> interrogate()
```

::: callout-tip
Click the blue "CSV" buttons above to download a .csv file of just the rows that failed that particular validation
:::

### Flexible validations

If a validation function you need doesn't exist, you can use `col_vals_expr()`

E.g. let's add a validation to check that `hoppers`is always a whole number.

```{r}
#| column: body-outset
agent_informed <-
  agent_informed |> 
  col_vals_expr(~ hoppers %% 1 == 0) 

agent_informed |> interrogate()
```

### Check that shoots are growing

The tea shoots should be consistently growing (within measurement error), so we can use `col_vals_increasing()` to check that shoot height is always going up.
We use the `segment` argument to specify that this check happens *within* each `plant_id`

```{r}
agent_informed <-
  agent_informed |> 
  #just with one shoot for demo purposes, but could use starts_with("shoot_")
  col_vals_increasing(shoot_1, segments = vars(plant_id))

agent_informed |> interrogate()
```

### Publishing validation reports

-   Students, faculty, and staff at University of Arizona have access to **RStudio Connect** which allows you to publish an RMarkdown document to the web with a single click.
    ([Learn More](https://datascience.arizona.edu/analytics-powerhouse/rstudio-connect))

-   Data validation can be automated in a variety of ways.
    If you are interested in more advanced applications of data validation for your lab, [contact us](https://cct.cals.arizona.edu/contact)!

## Fixing mistakes

-   For true mistakes in data entry (paper -\> spreadsheet), probably ok to just edit raw data

-   For other errors, best practice:

    -   **Don't edit raw data!**
    -   Record all changes to raw data (e.g. by using an R script to make them)
    -   Flag observations that have been changed
    -   Publish raw data, cleaning steps/scripts, and "cleaned" data

-   Keep an eye out for future workshops on data wrangling/tidying/cleaning

## Help

Feel free to drop by the [CCT Data Science Team office hours](https://datascience.cals.arizona.edu/office-hours), which happens every Tuesday morning.
We would love to help you with your R questions about date/time, and more!

You can also [make an appointment with Eric](https://calendar.google.com/calendar/appointments/schedules/AcZssZ0xpzu3Nlo0FCIAjFFktPFzyl5Aup9-1yErSy9y8_Iq2P6DWisiYc8PhUnn6l5z74D3OtgTRPSm) to discuss this content and get troubleshooting help.
