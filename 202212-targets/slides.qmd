---
title: "Managing complicated research workflows in R with {targets}"
author: Eric R. Scott
format: 
  revealjs:
    fontsize: 1.9em
    logo: "img/logo.png"
    theme: 
    - "_extensions/cct-datascience/uaz/theme.scss"
    - "custom.scss"
editor: visual
---

## Before we get started

```{r}
#| echo: false
library(countdown)
library(targets)
```

You'll need the following R packages today:

```{r}
#| eval: false
#| echo: true

#For making `targets` work:
library(targets)
library(tarchetypes)
library(visNetwork)
library(future) #optional
library(future.callr) #optional

#For our data wrangling and analysis:
library(tidyverse)
library(janitor)
library(car)
library(broom)

#for installing demos and course materials:
library(usethis)

```

## Learning Objectives

::: incremental
-   Understand what problems `targets` (and workflow managers in general) solve

-   Be able to set up a simple project using `targets`, view the dependency graph, and run the workflow

-   Write a (good enough) custom R function

-   Have an awareness of the possibilities: parallelization, cloud computing, branching, Bayesian analyses

-   Know where to go for help with `targets`
:::

# Part 1: Context

## Moving Toward Reproducibility

Working toward reproducible analyses benefits:

-   You in the future

-   Your collaborators

-   The greater community

![Image from: Illustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst](img/horst-repro.jpg){fig-alt="Cute fuzzy monsters putting rectangular data tables onto a conveyor belt. Along the conveyor belt line are different automated “stations” that update the data, reading “WRANGLE”, “VISUALIZE”, and “MODEL”. A monster at the end of the conveyor belt is carrying away a table that reads “Complete analysis.”" fig-align="center"}

::: notes
How many have taken a workshop on reproduciblity?
:::

##  {#scenario1-1}

![](img/scenario1-1.png){fig-align="center"}

##  {#scenario1-2}

![](img/scenario1-2.png){fig-align="center"}

##  {#scenario1-3}

![](img/scenario1-3.png){fig-align="center"}

##  {#scenario1-4}

![](img/scenario1-4.png){fig-align="center"}

##  {#scenario2-1}

![](img/scenario2-1.png){fig-align="center"}

##  {#scenario2-2}

![](img/scenario2-2.png){fig-align="center"}

##  {#scenario2-3}

![](img/scenario2-3.png){fig-align="center"}

## Workflow management

::: columns
::: {.column width="70%"}
-   Automatically detect dependencies

-   Run your entire analysis with one master command

-   Skip steps that don't need to be re-run

-   Scaleable
:::

::: {.column width="30%"}
![](https://docs.ropensci.org/targets/logo.svg){fig-align="center" width="193"}
:::
:::

![](img/pipeline_graph.png){fig-align="center" width="722"}

::: notes
Similar to make (language agnostic), or snakemake (for Python)\
scaleable = able to run with multiple cores, on HPC, using cloud storage
:::

# Part 2: Demonstration

## Demo

To install a demo `targets` pipeline, run the following R code and follow the prompts:

``` r
usethis::use_course("cct-datascience/targets-demo")
```

::: notes
1.  tar_make()
2.  examine outputs (.docx and with tar_read())
3.  examine \_targets.R & R/
4.  tar_visnetwork()
5.  make a change
6.  tar_visnetwork()
7.  tar_make()
8.  Show \_targets/ and discuss
:::

## Working with targets

-   Every intermediate (called a "target") is stored as an R object[^1]

-   Everything that happens to make that target is written as a function

    ```{r}
    #| fig-height: 4 
    tar_dir({
      tar_script({
        tar_option_set()
        read_wrangle_data <- function(data_file) {
          1
        }
        fit_model <- function(data) {
          2
        }
        make_table <- function(data, model) {
          3
        }
        list(
          tar_target(data_file, ""),
          tar_target(data, read_wrangle_data(data_file)),
          tar_target(model, fit_model(data)),
          tar_target(table, make_table(data, model))
        )
      })
      tar_make(reporter = "silent")
      tar_visnetwork()
    })
    ```

## Anatomy of a targets project

1.  `_targets.R`
    -   Configure and define workflow
2.  `R/`
    -   R script(s) with custom functions
3.  `_targets/`
    1.  Generated by `tar_make()`

    2.  Contains targets saved as R objects

    3.  Should ***not*** be tracked in version control

# Part 3: Refactoring Exercise

## Refactoring a project to use targets

> **refactor**
>
> /rēˈfaktər/
>
> *verb*
>
> 1.  restructure (the source code of an application or piece of software) so as to improve operation without altering functionality.

First, let's start with a "traditional" R analysis project.
Download and take a look around:

``` r
usethis::use_course("cct-datascience/targets-refactor")
```

```{r}
countdown(minutes = 3, right = "33.33%", color_background = "#001C48")
```

## Refactoring a project to use targets

::: incremental
1.  `use_targets()` to set up infrastructure including `_targets.R`
2.  Add package dependencies to `tar_option_set()`
3.  Convert R script(s) to functions
4.  Write targets
5.  Run `tar_make()`
6.  Debug
:::

. . .

**We're going to walk through this process together**

## 1. `use_targets()`

Run `use_targets()` and open the `_targets.R` file it generates

```{r}
#| eval: false
#| echo: true
library(targets)
use_targets()
```

## 2. Package dependencies

Figure out what packages are needed and add them to `tar_option_set()` in `_targets.R`.

```{r}
#| filename: _targets.R
#| eval: false
#| echo: true
tar_option_set(
  packages = c(
    "tidyverse",
    "lubridate",
    #add more packages here
    ),
  format = "rds"
)
```

```{r}
countdown(minutes = 2, color_background = "#001C48")
```

## 3. Create functions

Functions in R are created with the `function()` function

```{r}
#| echo: true
#| code-line-numbers: "1-6|1|2|3|4|5-6"
add_ten <-        #name of function
  function(x) {   #argument names and defaults defined here
    x + 10        #what the function does
  }

add_ten(x = 13)
```

. . .

Using multiple arguments:

```{r}
#| echo: true
join_data <- 
  function(weather_data, field_data) {
    #get dates into same format
    field_data_clean <- field_data %>%
      mutate(date = lubridate::mdy(date))
    
    #join to weather data
    left_join(field_data_clean, weather_data, by = "date")
  }
```

. . .

::: callout-important
The last line of a function must return something, so don't end a function with an assignment!
:::

## "Good enough" functions for targets

-   Use a naming convention (verbs are good)
-   Make the function arguments the same as target names
-   Document!

**BAD:**

``` r
m1 <- function(x) {
  lm(stem_length ~ watershed, data = x)
}
```

**GOOD:**

``` r
# Fit linear model to explain stem lenght as a function of watershed treatment
# data_clean = a tibble; the `data_clean` target
fit_model <- function(data_clean) {
  lm(stem_length ~ watershed, data = data_clean)
}
```

## 4. Create targets

Targets are defined by the `tar_target()` function inside of the `list()` at the end of `_targets.R`

```{r}
#| echo: true
#| eval: false
#| filename: _targets.R
list(
  tar_target(name = file, command = "data/penguins_raw.csv", format = "file"),
  tar_target(name = data, command = get_data(file))
)
```

. . .

-   Usually you only need to use the first two arguments, `name` for the name of the target, and `command` for what that target does

-   Targets that are files (or create files) need the additional argument `format = "file"` (for files on disk) or `format = "url"` (for files on the web)

. . .

::: callout-tip
A common mistake is to leave a trailing comma after the last target.
This will result in the error: `Error running targets::tar_make() Last error: argument 5 is empty`
:::

## Naming targets

-   Use a naming convention (nouns are good)

-   Use concise but descriptive target names

**BAD:**

-   `data1`, `data2`, `data3`
-   `histogram_by_site_plot`

**GOOD:**

-   `data_file`, `data_raw`, `data_clean`

-   `plot_hist_site`

## Pair-programming Exercise

-   Breakout groups of 2
-   One person shares screen and "drives"
-   The other person "navigates"

::: callout-tip
Try starting with a target for the path to the data file.
Remember to use `format = "file"` in `tar_target()`.

Check your progress by running `tar_visnetwork()` and `tar_make()` frequently!
:::

<!-- hmm.. will the navigator feel like they miss out because they won't have a copy of the code at the end?  We can share the "solution" with everyone -->

```{r}
countdown(minutes = 10, color_background = "#001C48")
```

## Debugging targets

-   Errors in `tar_make()` are sometimes uninformative because code is run in a separate R session
-   Use `tar_meta()` to access error messages

```{r}
#| eval: false
#| echo: true
tar_meta(fields = error, complete_only = TRUE)
```

## Debugging targets

If the error is in a function you wrote:

1.  Use `tar_load(target_name)` to load necessary targets into environment
2.  Use `tar_load_global()` to load functions and settings
3.  Test your function in the console
4.  Test function line by line

```{r}
#| echo: true
#| eval: false

tar_load_global()
tar_load(data_clean)

fit_model(data_clean)
```

```{r}
#| error: true
fit_model <- function(data_clean) {
  lm(y ~ x + z, data = df_cleaned)
}
fit_model(data_clean)
```

. . .

```{r}
#| filename: fit_model.R
#| echo: true
#| error: true
#| code-line-numbers: "2"

fit_model <- function(data_clean) {
  lm(y ~ x + z, data = df_cleaned)
}
```

```{r}
#| error: true
fit_model(data_clean)
```

## Tarchetypes

<!--# Maybe move this to end of Part 3 and expand a bit? -->

The `tarchetypes` package includes shortcuts and helpers to make `targets` easier to write and use.

```{r}
#| filename: _targets.R
#| eval: false
#| echo: true
#| code-line-numbers: "2|6|7|8|9|10"
library(targets)
library(tarchetypes)

tar_options_set()
tar_source()
tar_plan(                                    #<- alternative to list()
  tar_file(data_raw, "mydata.csv"),          #<- shortcut tar_* functions
  data = read_csv(data_raw),                 #<- target_name = command
  report = tar_render(report, "report.Rmd"), #<- render RMarkdown or Quarto
  model = fit_model(data),                   #<- trailing comma is OK!
)
```

<!-- At this point, maybe more time for debugging?? -->

# Part 4: A Taste of What's Possible

## Bayesian analyses with targets

::: columns
::: column
[![](https://docs.ropensci.org/stantargets/logo.svg){fig-align="center" width="250"}](https://docs.ropensci.org/stantargets/)
:::

::: column
[![](https://docs.ropensci.org/jagstargets/logo.svg){fig-align="center" width="250"}](https://docs.ropensci.org/jagstargets/)
:::
:::

"target factories" for Bayesian analyses

-   Simplify analyses by automatically creating targets for multiple steps

-   E.g. defining a target with `tar_stan_mcmc()` actually generates multiple targets that wrangle data, run the MCMC, create a table of posterior draws, etc.

## Parallel computation

In the workflow below, the three models and the three plot targets can all be run independently at the same time.

![](img/pipeline_graph.png){fig-align="center" width="722"}

-   `tar_make_future()` uses the `future` package and `tar_make_clustermq()` uses the `clustermq` package to run targets in parallel!

-   Detailed discussion of pros and cons of both are in [the manual](https://books.ropensci.org/targets/hpc.html)

::: notes
Demo by creating 3 dummy targets with Sys.sleep(5) and running with tar_make() vs. tar_make_clustermq().\

\
The refactor exercise is NOT a good example for this because the overhead for both clustermq and future (especially future) makes the pipeline take \*longer\* to run in paralell.
Something to discuss?
:::

## Branching

> Sometimes, a pipeline contains more targets than a user can comfortably type by hand.
> For projects with hundreds of targets, branching can make the code in \_targets.R shorter and more concise.

::: incremental
-   Branching lets you break repetitive tasks into separate targets for increased speed from parallelization

-   E.g. fitting model to bootstraps of data
:::

. . .

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "1-6|7-13"

  #creates list of bootstrapped dataframes
  tar_target(
    data_boot,
    purrr::map(1:100, ~sample_frac(data, size = 1, replace = TRUE)),
    iteration = "list"
  ),
  # Fit model to each data frame, save results as a list
  tar_target(
    lm_boot,
    fit_lm_full(data_boot),
    pattern = map(data_boot),
    iteration = "list"
  ),

```

::: notes
Use the demo repo to demo this!
Show `tar_visnetwork()` and `tar_make()` and `tar_read(lm_boot, branches = 1)`
:::

## High Performance Computing

-   *Relatively* easy to get a targets pipeline working on an HPC

-   Targets are run on persistent (`clustermq`) or transient (`future`) SLURM jobs and results collected

-   Best option at UAZ (IMO) is to use [Open On Demand](https://ood.hpc.arizona.edu/)

    -   Runs RStudio in a browser window using the HPC

    -   Select number of cores ≥ number of workers

    -   Use either `tar_make_clustermq()` or `tar_make_future()`

::: callout-warning
Running `use_targets()` on an Open On Demand session won't get the setup quite right.
Use `options(clustermq.scheduler = "multicore")` or `future::plan(future.callr::callr)` just as we did above.
:::

## Cloud Storage

-   By default, the `_targets/` store is on your computer and not shared with collaborators

-   Collaborators will have to run `tar_make()` to reproduce the workflow, which might not be convenient if some targets take days or weeks to run

![](img/cloud-1.png){fig-alt="Diagram describing how the _targets/ folder is stored locally and not synced to GitHub (only _targets.R and the R folder are on GitHub).  A collaborator therefore doesn't have the _targets folder and needs to run tar_make() on their computer." fig-align="center"}

::: notes
-   `_targets/` folder is by default on **your** hard drive

-   Your collaborator can reproduce the whole workflow from the start with `tar_make()`, but what if that takes *days* and all they want to do is make changes at the end of the workflow?

-   Solution: store \_targets/ in the cloud with [Amazon Web Services](https://books.ropensci.org/targets/data.html#aws-setup) or [Google Cloud](https://books.ropensci.org/targets/data.html#gcp-setup)

-   BONUS: targets stored in this way can be *versioned,* so if you roll back your `_targets.R` to a previous version, you don't have to re-run all the targets.
:::

## Cloud Storage

-   Optionally, `_targets/` can be stored in the cloud ([Amazon Web Services](https://books.ropensci.org/targets/data.html#aws-setup) or [Google Cloud](https://books.ropensci.org/targets/data.html#gcp-setup) S3 buckets)

-   These stores can be versioned, so you can roll back your `_targets.R` and not have to re-compute targets.

![](img/cloud-2.png){fig-alt="Variation of previous diagram but now with _targets/ stored in a cloud that is accessible to both computers" fig-align="center"}

## Wrap-up

**When to use `targets`?**

. . .

Things to consider:

::: incremental
-   Benefits of paralellization

-   Your collaborators

-   Your comfort using `targets`
:::

## Targets in the wild

**"Real life" examples of `targets` workflows:**

-   <https://github.com/BrunaLab/lagged-ipms> (uses HPC to run dynamic branches in parallel)
-   <https://github.com/odaniel1/track-cycling> (uses `stantargets`)
-   <https://github.com/ecohealthalliance/mpx-diagnosis> (medium complexity)
-   <https://github.com/noamross/reprotemplate> (nice template for reproducibility that uses `targets`)

## Where to go for help

-   `targets` manual: <https://books.ropensci.org/targets/>

-   `targets` reference: <https://docs.ropensci.org/targets/>

-   `targets` discussion board: <https://github.com/ropensci/targets/discussions>

-   [CCT Data Science Team office hours](https://datascience.cals.arizona.edu/office-hours) every Tuesday morning

-   [Make an appointment with Eric](https://calendar.google.com/calendar/appointments/schedules/AcZssZ0xpzu3Nlo0FCIAjFFktPFzyl5Aup9-1yErSy9y8_Iq2P6DWisiYc8PhUnn6l5z74D3OtgTRPSm) to discuss this content and get troubleshooting help

-   Sign up for our [mailing list](https://app.e2ma.net/app2/audience/signup/1965047/1923145/) to be notified about future workshops

[^1]: The default is to save targets as .rds files, but [other options](https://books.ropensci.org/targets/performance.html#efficient-storage-formats) are available.
