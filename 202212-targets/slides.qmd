---
title: "Managing complicated research workflows in R with {targets}"
author: Eric R. Scott
format: uaz-revealjs
editor: visual
---

<!--# TODO:- Make font size smaller- Switch logo to cct-datascience- Add images to replace text- Add timer widget to exercise slide- Acknowledge Will Landau for inspiration for slides- Add part 4 slides (branching, parallel computing, cloud storage exist) -->

## Before we get started

You'll need the following R packages today:

```{r}
#| eval: false
#| echo: true

#For making `targets` work:
library(targets)
library(tarchetypes)
library(visNetwork)
library(future)
library(future.callr)

#For our data wrangling and analysis:
library(tidyverse)
library(lubridate)

#for installing demos and course materials:
library(usethis)

```

## Moving Toward Reproducibility

-   Reproducibility vs. replicability

-   Why bother?

    -   For you in the future

    -   For your collaborators

    -   For the greater community

    <!--# Look at Jenny Bryan's slides maybe and see if there are good images to use -->

##  {#scenario1-1}

![](img/scenario1-1.png){fig-align="center"}

##  {#scenario1-2}

![](img/scenario1-2.png){fig-align="center"}

##  {#scenario1-3}

![](img/scenario1-3.png){fig-align="center"}

##  {#scenario1-4}

![](img/scenario1-4.png){fig-align="center"}

##  {#scenario2-1}

![](img/scenario2-1.png){fig-align="center"}

##  {#scenario2-2}

![](img/scenario2-2.png){fig-align="center"}

##  {#scenario2-3}

![](img/scenario2-3.png){fig-align="center"}

## 

## Workflow management

::: columns
::: {.column width="70%"}
-   Automatically detect dependencies

-   Run your entire analysis with one master command

-   Skip steps that don't need to be re-run
:::

::: {.column width="30%"}
![](https://docs.ropensci.org/targets/logo.svg){fig-align="center" width="193"}
:::
:::

![](img/pipeline_graph.png){fig-align="center" width="722"}

## Demo

To install a demo `targets` pipeline, run the following R code and follow the prompts:

``` r
usethis::use_course("cct-datascience/targets-demo")
```

::: notes
1.  tar_make()
2.  examine outputs (.docx and with tar_read())
3.  examine \_targets.R & R/
4.  tar_visnetwork()
5.  make a change
6.  tar_visnetwork()
7.  tar_make()
8.  Show \_targets/ and discuss
:::

## Working with `targets`

-   Every step ("target") is an R object[^1]

-   Everything that happens to make that target is a function

    <!--# add image of example workflow -->

## Anatomy of a `targets` project

1.  \_targets.R
    -   Configure and define workflow
2.  R/
    -   R script(s) with custom functions
3.  \_targets/
    1.  Generated by `tar_make()`

    2.  Contains targets saved as R objects

    3.  Should ***not*** be tracked in version control

## Starting a new `targets` project

Use `use_targets()` to set up a brand new project, or convert an existing one to use `targets` .

::: callout-tip
Follow along on your own computer for best learning results!
:::

::: notes
Use .csv that's online so they don't need to download data and demo `tar_url()` ---carpentries maybe?

Have a question to answer so you can write some simple data wrangling functions

Demo sketching out target names at start

Demo sketching out code, transfer to function, run `tar_make()` workflow.
:::

## Refactoring a project to use `targets`

> **refactor**
>
> /rēˈfaktər/
>
> *verb*
>
> 1.  restructure (the source code of an application or piece of software) so as to improve operation without altering functionality.

## Refactoring a project to use `targets`

### To-do:

1.  `use_targets()` to set up infrastructure including `_targets.R`
2.  Convert R script(s) to functions
3.  Write targets
4.  Run `tar_make()`
5.  Debug

Best to do this one target at a time

## Writing functions for `targets`

-   Use a naming convention (verbs are good)
-   Make the function arguments the same as target names
-   The last step usually must *return* something

**BAD:**

``` r
m1 <- function(x) {
  lm(stem_length ~ watershed, data = x)
}
```

**GOOD:**

``` r
fit_model <- function(data_clean) {
  lm(stem_length ~ watershed, data = data_clean)
}
```

## Creating targets

-   Use a naming convention (nouns are good)

-   Use concise but descriptive target names

**BAD:**

-   `data1`, `data2`, `data3`
-   `histogram_by_site_plot`

**GOOD:**

-   `data_file`, `data_raw`, `data_clean`

-   `plot_hist_site`

## Debugging targets

-   Errors in `tar_make()` are usually uninformative because code is run in a separate R session with `callr`.
-   Use `tar_meta()` to access error messages

``` r
tar_meta(fields = error, complete_only = TRUE)
```

## Debugging targets

If the error is in a custom function:

1.  Use `tar_load()` to load necessary targets into environment
2.  Test function in console
3.  Run through code in function interactively

::: notes
Demo this by purposefully introducing an error in the demo project
:::

## Exercise {.small}

1.  Install another demo project

``` r
usethis::use_course("cct-datascience/targets-refactor")
```

2.  *Refactor* the project to use `targets`

    -   Run `use_targets()` and create a `tar_plan()`
    -   Turn scripts or portions of scripts into functions in the `R/` folder
    -   Add packages to `tar_option_set(packages = c(…))`
    -   Check workflow with `tar_visnetwork()`
    -   Checking if it works with `tar_make()`
    -   Debug functions if necessary

    ### 

[^1]: The default is to save targets as .rds files, but [other options](https://books.ropensci.org/targets/performance.html#efficient-storage-formats) are available.
