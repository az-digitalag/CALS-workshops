---
title: "Managing complicated research workflows in R with {targets}"
author: Eric R. Scott
format: 
  uaz-revealjs:
    fontsize: 1.9em
editor: visual
---

```{r}
#| echo: false
library(countdown)
library(targets)
```

## Before we get started

You'll need the following R packages today:

```{r}
#| eval: false
#| echo: true

#For making `targets` work:
library(targets)
library(tarchetypes)
library(visNetwork)
library(future)
library(future.callr)

#For our data wrangling and analysis:
library(tidyverse)
library(lubridate)

#for installing demos and course materials:
library(usethis)

```

# Part 1: Context

## Moving Toward Reproducibility

-   Reproducibility vs. replicability

-   Why bother?

    -   For you in the future

    -   For your collaborators

    -   For the greater community

    <!--# Look at Jenny Bryan's slides maybe and see if there are good images to use -->

##  {#scenario1-1}

![](img/scenario1-1.png){fig-align="center"}

##  {#scenario1-2}

![](img/scenario1-2.png){fig-align="center"}

##  {#scenario1-3}

![](img/scenario1-3.png){fig-align="center"}

##  {#scenario1-4}

![](img/scenario1-4.png){fig-align="center"}

##  {#scenario2-1}

![](img/scenario2-1.png){fig-align="center"}

##  {#scenario2-2}

![](img/scenario2-2.png){fig-align="center"}

##  {#scenario2-3}

![](img/scenario2-3.png){fig-align="center"}

## Workflow management

::: columns
::: {.column width="70%"}
-   Automatically detect dependencies

-   Run your entire analysis with one master command

-   Skip steps that don't need to be re-run
:::

::: {.column width="30%"}
![](https://docs.ropensci.org/targets/logo.svg){fig-align="center" width="193"}
:::
:::

![](img/pipeline_graph.png){fig-align="center" width="722"}

# Part 2: Demonstration

## Demo

To install a demo `targets` pipeline, run the following R code and follow the prompts:

``` r
usethis::use_course("cct-datascience/targets-demo")
```

::: notes
1.  tar_make()
2.  examine outputs (.docx and with tar_read())
3.  examine \_targets.R & R/
4.  tar_visnetwork()
5.  make a change
6.  tar_visnetwork()
7.  tar_make()
8.  Show \_targets/ and discuss
:::

## Working with `targets`

-   Every intermediate (called a "target") is stored as an R object[^1]

-   Everything that happens to make that target is written as a function

    ```{r}
    #| fig-height: 4 
    tar_dir({
      tar_script({
        tar_option_set()
        read_wrangle_data <- function(data_file) {
          1
        }
        fit_model <- function(data) {
          2
        }
        make_table <- function(data, model) {
          3
        }
        list(
          tar_target(data_file, ""),
          tar_target(data, read_wrangle_data(data_file)),
          tar_target(model, fit_model(data)),
          tar_target(table, make_table(data, model))
        )
      })
      tar_make(reporter = "silent")
      tar_visnetwork()
    })
    ```

## Anatomy of a `targets` project

1.  \_targets.R
    -   Configure and define workflow
2.  R/
    -   R script(s) with custom functions
3.  \_targets/
    1.  Generated by `tar_make()`

    2.  Contains targets saved as R objects

    3.  Should ***not*** be tracked in version control

# Part 3: Exercise

## Refactoring a project to use `targets`

> **refactor**
>
> /rēˈfaktər/
>
> *verb*
>
> 1.  restructure (the source code of an application or piece of software) so as to improve operation without altering functionality.

First, let's start with a "traditional" R analysis project.
Download and take a look around:

``` r
usethis::use_course("cct-datascience/targets-refactor")
```

```{r}
countdown(minutes = 5, right = "33.33%")
```

## Refactoring a project to use `targets`

### To-do:

1.  `use_targets()` to set up infrastructure including `_targets.R`
2.  Add package dependencies to `tar_option_set()`
3.  Convert R script(s) to functions
4.  Write targets
5.  Run `tar_make()`
6.  Debug

Best to do steps 3-6 **one target at a time!**

## 1. `use_targets()`

Run `use_targets()` and open the `_targets.R` file it generates

```{r}
#| eval: false
#| echo: true
library(targets)
use_targets()
```

## 2. Package dependencies

Figure out what packages are needed and add them to `tar_option_set()` in `_targets.R`.

```{r}
#| eval: false
#| echo: true
tar_option_set(
  packages = c(
    "tidyverse",
    "lubridate",
    #add more packages here
    ),
  format = "rds"
)
```

```{r}
countdown(minutes = 2)
```

## 3. Create functions

Functions in R are created with the `function()` function

```{r}
#| echo: true
#| code-line-numbers: "1-6|1|2|3|4|5-6"
add_ten <-        #name of function
  function(x) {   #argument names and defaults defined here
    x + 10        #what the function does
  }

add_ten(x = 13)
```

. . .

Using multiple arguments:

```{r}
#| echo: true
join_data <- 
  function(weather_data, field_data) {
    #get dates into same format
    field_data_clean <- field_data %>%
      mutate(date = lubridate::mdy(date))
    
    #join to weather data
    left_join(field_data_clean, weather_data, by = "date")
  }
```

## "Good enough" functions for `targets`

-   Use a naming convention (verbs are good)
-   Make the function arguments the same as target names
-   The last step usually must *return* something

**BAD:**

``` r
m1 <- function(x) {
  lm(stem_length ~ watershed, data = x)
}
```

**GOOD:**

``` r
fit_model <- function(data_clean) {
  lm(stem_length ~ watershed, data = data_clean)
}
```

## 4. Create targets

-   Use a naming convention (nouns are good)

-   Use concise but descriptive target names

**BAD:**

-   `data1`, `data2`, `data3`
-   `histogram_by_site_plot`

**GOOD:**

-   `data_file`, `data_raw`, `data_clean`

-   `plot_hist_site`

## Creating targets for data

-   Use `format="file"` for targets that point to raw data on disk

-   Use `format="url"` for targets that point to a file on the web

-   This will track changes in the data and mark downstream targets as out of date when the data changes

```{r}
#| eval: false
#| echo: true
list(
  tar_target(data_raw, "data/mydata.csv", format = "file"),
  tar_target(data, read_wrangle_data(data_raw))
)
```

## Pair-programming Exercise

-   Breakout groups of 2
-   One person shares screen and "drives"
-   The other person "navigates"

::: callout-tip
Try starting with a target for the path to the data file.
Remember to use `format = "file"` in `tar_target()`.

Check your progress by running `tar_visnetwork()` and `tar_make()` frequently!
:::

<!-- hmm.. will the navigator feel like they miss out because they won't have a copy of the code at the end?  We can share the "solution" with everyone -->

```{r}
countdown(minutes = 10)
```

## Debugging targets

-   Errors in `tar_make()` are sometimes uninformative because code is run in a separate R session
-   Use `tar_meta()` to access error messages

``` r
tar_meta(fields = error, complete_only = TRUE)
```

## Debugging targets

If the error is in a function you wrote:

1.  Use `tar_load("target_name")` to load necessary targets into environment
2.  Use `tar_load_global()` to load functions and settings
3.  Test your function in the console
4.  Test function line by line

```{r}
#| echo: true
#| eval: false

tar_load_global()
tar_load(data_clean)

fit_model(data_clean)
```

```{r}
#| error: true
fit_model <- function(data_clean) {
  lm(y ~ x + z, data = df_cleaned)
}
fit_model(data_clean)
```

. . .

```{r}
#| filename: fit_model.R
#| echo: true
#| error: true
#| code-line-numbers: "2"

fit_model <- function(data_clean) {
  lm(y ~ x + z, data = df_cleaned)
}
```

```{r}
#| error: true
fit_model(data_clean)
```

<!-- At this point, maybe more time for debugging?? -->

# Part 4: A Taste of What's Possible

## Bayesian analyses with `targets`

::: columns
::: column
[![](https://docs.ropensci.org/stantargets/logo.svg){fig-align="center" width="300"}](https://docs.ropensci.org/stantargets/)
:::

::: column
[![](https://docs.ropensci.org/jagstargets/logo.svg){fig-align="center" width="300"}](https://docs.ropensci.org/jagstargets/)
:::
:::

"target factories" for Bayesian analyses

-   Simplify analyses by automatically creating targets for multiple steps

-   E.g. defining a target with `tar_stan_mcmc()` actually generates multiple targets that wrangle data, run the MCMC, create a table of posterior draws, etc.

## Tarchetypes

The `tarchetypes` package includes shortcuts and helpers to make `targets` easier to write and use.

```{r}
#| filename: _targets.R
#| eval: false
#| echo: true
library(targets)
library(tarchetypes)

tar_plan(                                    #<- alternative to list()
  tar_file(data_raw, "mydata.csv"),          #<- shortcut tar_* functions
  data = read_csv(data_raw),                 #<- target_name = command
  report = tar_render(report, "report.Rmd"), #<- render RMarkdown or Quarto
  model = fit_model(data),                   #<- trailing comma is OK!
)
```

## Parallel computation

In the workflow below, the three models and the three plot targets can all be run independently at the same time.

![](img/pipeline_graph.png){fig-align="center" width="722"}

<!--# The refactor exercise is NOT a good example for this because the overhead for both clustermq and future (especially future) makes the pipeline take *longer* to run in paralell.  Something to discuss? -->

-   `tar_make_future()` uses the `future` package and `tar_make_clustermq()` uses the `clustermq` package to run targets in parallel!

-   Detailed discussion of pros and cons of both are in [the manual](https://books.ropensci.org/targets/hpc.html)

```{r}
#| filename: _targets.R
#| eval: false
#| echo: true
# clustermq settings
options(clustermq.scheduler = "multicore")

#future settings
future::plan(future.callr::callr)
```

```{r}
#| filename: R console
#| eval: false
#| echo: true

tar_make_future(workers = 3)
tar_make_clustermq(workers = 3)
```

## Branching

## High Performance Computing

-   *Relatively* easy to get a targets pipeline working on an HPC

-   Targets are run on persistent (`clustermq`) or transient (`future`) SLURM jobs and results collected

-   Best option (IMO) is to use [Open On Demand](https://ood.hpc.arizona.edu/)

    -   Runs RStudio in a browser window using the HPC

    -   Select number of cores ≥ number of workers

    -   Use either `tar_make_clustermq()` or `tar_make_future()`

::: callout-warning
Running `use_targets()` on an Open On Demand session won't get the setup quite right.
Use `options(clustermq.scheduler = "multicore")` or `future::plan(future.callr::callr)` just as we did above.
:::

## Cloud Storage

-   \_targets/ folder is by default on your hard drive

-   Workflow is reproducible by collaborator, but why should they have to wait for `tar_make()` to finish when you've already run it?

-   Solution: store \_targets/ in the cloud with Amazon S3 buckets or Google Cloud Storage

-   Targets stored in this way can be *versioned,* so if you roll back your `_targets.R` to a previous version, you don't have to re-run all the targets.

[^1]: The default is to save targets as .rds files, but [other options](https://books.ropensci.org/targets/performance.html#efficient-storage-formats) are available.
